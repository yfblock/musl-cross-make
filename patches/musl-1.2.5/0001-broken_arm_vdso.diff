diff --git musl-1.2.5.orig/arch/aarch64/syscall_arch.h musl-1.2.5/arch/aarch64/syscall_arch.h
--- musl-1.2.5.orig/arch/aarch64/syscall_arch.h	2024-03-01 10:07:33.000000000 +0800
+++ musl-1.2.5/arch/aarch64/syscall_arch.h	2024-08-11 20:00:53.100656017 +0800
@@ -1,74 +1,99 @@
 #define __SYSCALL_LL_E(x) (x)
 #define __SYSCALL_LL_O(x) (x)
 
+/**
+ * TODO: This is a simple solution for the current case.
+ *       Improvement of this is read reduce context operations.
+ */
+
+// #define __asm_syscall(...) do { \
+// 	__asm__ __volatile__ ( "svc 0" \
+// 	: "=r"(x0) : __VA_ARGS__ : "memory", "cc"); \
+// 	return x0; \
+// 	} while (0)
+// #define __asm_syscall(...) do { \
+// 	register void *saved_lr __asm__("x30"); \
+// 	__asm__ __volatile__ (\
+// 		"bl _vsyscall" \
+// 	: "=r"(x0) : __VA_ARGS__ ,"r"(saved_lr) : "memory", "cc"); \
+// 	return x0; \
+// 	} while (0)
+
+extern long _vsyscall(long n, long a, long b, long c, long d, long e, long f);
+
 #define __asm_syscall(...) do { \
-	__asm__ __volatile__ ( "svc 0" \
-	: "=r"(x0) : __VA_ARGS__ : "memory", "cc"); \
-	return x0; \
+	return _vsyscall(); \
 	} while (0)
 
 static inline long __syscall0(long n)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0");
-	__asm_syscall("r"(x8));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0");
+	// __asm_syscall("r"(x8));
+	return _vsyscall(n, 0, 0, 0, 0, 0, 0);
 }
 
 static inline long __syscall1(long n, long a)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	__asm_syscall("r"(x8), "0"(x0));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// __asm_syscall("r"(x8), "0"(x0));
+	return _vsyscall(n, a, 0, 0, 0, 0, 0);
 }
 
 static inline long __syscall2(long n, long a, long b)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	register long x1 __asm__("x1") = b;
-	__asm_syscall("r"(x8), "0"(x0), "r"(x1));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// register long x1 __asm__("x1") = b;
+	// __asm_syscall("r"(x8), "0"(x0), "r"(x1));
+	return _vsyscall(n, a, b, 0, 0, 0, 0);
 }
 
 static inline long __syscall3(long n, long a, long b, long c)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	register long x1 __asm__("x1") = b;
-	register long x2 __asm__("x2") = c;
-	__asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// register long x1 __asm__("x1") = b;
+	// register long x2 __asm__("x2") = c;
+	// __asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2));
+	return _vsyscall(n, a, b, c, 0, 0, 0);
 }
 
 static inline long __syscall4(long n, long a, long b, long c, long d)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	register long x1 __asm__("x1") = b;
-	register long x2 __asm__("x2") = c;
-	register long x3 __asm__("x3") = d;
-	__asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// register long x1 __asm__("x1") = b;
+	// register long x2 __asm__("x2") = c;
+	// register long x3 __asm__("x3") = d;
+	// __asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3));
+	return _vsyscall(n, a, b, c, d, 0, 0);
 }
 
 static inline long __syscall5(long n, long a, long b, long c, long d, long e)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	register long x1 __asm__("x1") = b;
-	register long x2 __asm__("x2") = c;
-	register long x3 __asm__("x3") = d;
-	register long x4 __asm__("x4") = e;
-	__asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3), "r"(x4));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// register long x1 __asm__("x1") = b;
+	// register long x2 __asm__("x2") = c;
+	// register long x3 __asm__("x3") = d;
+	// register long x4 __asm__("x4") = e;
+	// __asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3), "r"(x4));
+	return _vsyscall(n, a, b, c, d, e, 0);
 }
 
 static inline long __syscall6(long n, long a, long b, long c, long d, long e, long f)
 {
-	register long x8 __asm__("x8") = n;
-	register long x0 __asm__("x0") = a;
-	register long x1 __asm__("x1") = b;
-	register long x2 __asm__("x2") = c;
-	register long x3 __asm__("x3") = d;
-	register long x4 __asm__("x4") = e;
-	register long x5 __asm__("x5") = f;
-	__asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3), "r"(x4), "r"(x5));
+	// register long x8 __asm__("x8") = n;
+	// register long x0 __asm__("x0") = a;
+	// register long x1 __asm__("x1") = b;
+	// register long x2 __asm__("x2") = c;
+	// register long x3 __asm__("x3") = d;
+	// register long x4 __asm__("x4") = e;
+	// register long x5 __asm__("x5") = f;
+	return _vsyscall(n, a, b, c, d, e, f);
+	// __asm_syscall("r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3), "r"(x4), "r"(x5));
 }
 
 #define VDSO_USEFUL
diff --git musl-1.2.5.orig/src/custom/aarch64/vsyscall.c musl-1.2.5/src/custom/aarch64/vsyscall.c
--- musl-1.2.5.orig/src/custom/aarch64/vsyscall.c	1970-01-01 08:00:00.000000000 +0800
+++ musl-1.2.5/src/custom/aarch64/vsyscall.c	2024-08-11 19:52:20.750738344 +0800
@@ -0,0 +1,15 @@
+#include <features.h>
+
+long vsyscall(long n, long a, long b, long c, long d, long e, long f) {
+    register long x8 __asm__("x8") = n;
+	register long x0 __asm__("x0") = a;
+	register long x1 __asm__("x1") = b;
+	register long x2 __asm__("x2") = c;
+	register long x3 __asm__("x3") = d;
+	register long x4 __asm__("x4") = e;
+	register long x5 __asm__("x5") = f;
+    __asm__ __volatile__("svc 0": "=r"(x0): "r"(x8), "0"(x0), "r"(x1), "r"(x2), "r"(x3), "r"(x4), "r"(x5):"memory", "cc");
+    return x0;
+}
+
+weak_alias(vsyscall, _vsyscall);
\ No newline at end of file
diff --git musl-1.2.5.orig/src/custom/aarch64/vsyscall.s.bak musl-1.2.5/src/custom/aarch64/vsyscall.s.bak
--- musl-1.2.5.orig/src/custom/aarch64/vsyscall.s.bak	1970-01-01 08:00:00.000000000 +0800
+++ musl-1.2.5/src/custom/aarch64/vsyscall.s.bak	2024-08-11 19:38:38.512962550 +0800
@@ -0,0 +1,6 @@
+.text
+.weak _vsyscall
+.type   _vsyscall,%function
+_vsyscall:
+    svc 0
+    ret
